{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68e21aa1",
   "metadata": {},
   "source": "# Avalia√ß√£o de Agentes\n\nTemos um assistente de email que usa um roteador para triagem de emails e ent√£o passa o email para o agente gerar uma resposta. Como podemos ter certeza de que funcionar√° bem em produ√ß√£o? √â por isso que os testes s√£o importantes: eles orientam nossas decis√µes sobre a arquitetura do agente com m√©tricas quantific√°veis como qualidade da resposta, uso de tokens, lat√™ncia ou precis√£o da triagem. O [LangSmith](https://docs.smith.langchain.com/) oferece duas maneiras principais de testar agentes.\n\n![overview-img](img/overview_eval.png)"
  },
  {
   "cell_type": "markdown",
   "id": "4d7f7048",
   "metadata": {},
   "source": "#### Carregar Vari√°veis de Ambiente"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c47d4c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2005c34d",
   "metadata": {},
   "source": "## Como Executar Avalia√ß√µes\n\n#### Pytest / Vitest\n\n[Pytest](https://docs.pytest.org/en/stable/) e Vitest s√£o bem conhecidos por muitos desenvolvedores como ferramentas poderosas para escrever testes dentro dos ecossistemas Python e JavaScript. O LangSmith se integra com esses frameworks para permitir que voc√™ escreva e execute testes que registram resultados no LangSmith. Para este notebook, usaremos Pytest.\n* Pytest √© uma √≥tima maneira de come√ßar para desenvolvedores que j√° est√£o familiarizados com o framework.\n* Pytest √© √≥timo para avalia√ß√µes mais complexas, onde cada caso de teste do agente requer verifica√ß√µes espec√≠ficas e crit√©rios de sucesso que s√£o mais dif√≠ceis de generalizar.\n\n#### Datasets do LangSmith\n\nVoc√™ tamb√©m pode criar um dataset [no LangSmith](https://docs.smith.langchain.com/evaluation) e executar nosso assistente contra o dataset usando a API de avalia√ß√£o do LangSmith.\n* Os datasets do LangSmith s√£o √≥timos para equipes que est√£o construindo colaborativamente sua su√≠te de testes.\n* Voc√™ pode aproveitar traces de produ√ß√£o, filas de anota√ß√£o, gera√ß√£o de dados sint√©ticos, e mais, para adicionar exemplos a um dataset dourado sempre crescente.\n* Os datasets do LangSmith s√£o √≥timos quando voc√™ pode definir avaliadores que podem ser aplicados a todos os casos de teste no dataset (ex. similaridade, precis√£o de correspond√™ncia exata, etc.)"
  },
  {
   "cell_type": "markdown",
   "id": "10b7c989",
   "metadata": {},
   "source": "## Casos de Teste\n\nOs testes geralmente come√ßam com a defini√ß√£o dos casos de teste, o que pode ser um processo desafiador. Neste caso, vamos apenas definir um conjunto de emails de exemplo que queremos gerenciar junto com algumas coisas para testar. Voc√™ pode ver os casos de teste em `eval/email_dataset.py`, que cont√©m o seguinte:\n\n1. **Emails de Entrada**: Uma cole√ß√£o de exemplos diversos de email\n2. **Classifica√ß√µes Verdadeiras**: `Respond`, `Notify`, `Ignore`\n3. **Chamadas de Ferramentas Esperadas**: Ferramentas chamadas para cada email que requer uma resposta\n4. **Crit√©rios de Resposta**: O que torna uma boa resposta para emails que requerem respostas\n\nNote que temos tanto:\n- Testes \"de integra√ß√£o\" ponta a ponta (ex. Emails de Entrada -> Agente -> Sa√≠da Final vs Crit√©rios de Resposta)\n- Testes para etapas espec√≠ficas em nosso fluxo de trabalho (ex. Emails de Entrada -> Agente -> Classifica√ß√£o vs Classifica√ß√£o Verdadeira)"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8fdc2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email Input: {'author': 'Alice Smith <alice.smith@company.com>', 'to': 'Lance Martin <lance@company.com>', 'subject': 'Quick question about API documentation', 'email_thread': \"Hi Lance,\\n\\nI was reviewing the API documentation for the new authentication service and noticed a few endpoints seem to be missing from the specs. Could you help clarify if this was intentional or if we should update the docs?\\n\\nSpecifically, I'm looking at:\\n- /auth/refresh\\n- /auth/validate\\n\\nThanks!\\nAlice\"}\n",
      "Expected Triage Output: respond\n",
      "Expected Tool Calls: ['write_email', 'done']\n",
      "Response Criteria: \n",
      "‚Ä¢ Send email with write_email tool call to acknowledge the question and confirm it will be investigated  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from email_assistant.eval.email_dataset import email_inputs, expected_tool_calls, triage_outputs_list, response_criteria_list\n",
    "\n",
    "test_case_ix = 0\n",
    "\n",
    "print(\"Email Input:\", email_inputs[test_case_ix])\n",
    "print(\"Expected Triage Output:\", triage_outputs_list[test_case_ix])\n",
    "print(\"Expected Tool Calls:\", expected_tool_calls[test_case_ix])\n",
    "print(\"Response Criteria:\", response_criteria_list[test_case_ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2337bd7c",
   "metadata": {},
   "source": "## Exemplo com Pytest\n\nVamos ver como podemos escrever um teste para uma parte espec√≠fica do nosso fluxo de trabalho com Pytest. Testaremos se nosso `email_assistant` faz as chamadas de ferramentas corretas ao responder aos emails."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae92fe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from email_assistant.eval.email_dataset import email_inputs, expected_tool_calls\n",
    "from email_assistant.utils import format_messages_string\n",
    "from email_assistant.email_assistant import email_assistant\n",
    "from email_assistant.utils import extract_tool_calls\n",
    "\n",
    "from langsmith import testing as t\n",
    "\n",
    "@pytest.mark.langsmith\n",
    "@pytest.mark.parametrize(\n",
    "    \"email_input, expected_calls\",\n",
    "    [   # Pick some examples with e-mail reply expected\n",
    "        (email_inputs[0],expected_tool_calls[0]),\n",
    "        (email_inputs[3],expected_tool_calls[3]),\n",
    "    ],\n",
    ")\n",
    "def test_email_dataset_tool_calls(email_input, expected_calls):\n",
    "    \"\"\"Test if email processing contains expected tool calls.\n",
    "\n",
    "    This test confirms that all expected tools are called during email processing,\n",
    "    but does not check the order of tool invocations or the number of invocations\n",
    "    per tool. Additional checks for these aspects could be added if desired.\n",
    "    \"\"\"\n",
    "    # Run the email assistant\n",
    "    messages = [{\"role\": \"user\", \"content\": str(email_input)}]\n",
    "    result = email_assistant.invoke({\"messages\": messages})\n",
    "\n",
    "    # Extract tool calls from messages list\n",
    "    extracted_tool_calls = extract_tool_calls(result['messages'])\n",
    "\n",
    "    # Check if all expected tool calls are in the extracted ones\n",
    "    missing_calls = [call for call in expected_calls if call.lower() not in extracted_tool_calls]\n",
    "\n",
    "    t.log_outputs({\n",
    "                \"missing_calls\": missing_calls,\n",
    "                \"extracted_tool_calls\": extracted_tool_calls,\n",
    "                \"response\": format_messages_string(result['messages'])\n",
    "            })\n",
    "\n",
    "    # Test passes if no expected calls are missing\n",
    "    assert len(missing_calls) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700aba2a",
   "metadata": {},
   "source": "Voc√™ notar√° algumas coisas:\n- Para [executar com Pytest e registrar resultados de teste no LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest), precisamos apenas adicionar o decorador `@pytest.mark.langsmith` √† nossa fun√ß√£o e coloc√°-la em um arquivo, como voc√™ v√™ em `notebooks/test_tools.py`. Isso registrar√° os resultados do teste no LangSmith.\n- Segundo, podemos passar exemplos de dataset para a fun√ß√£o de teste como mostrado [aqui](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#parametrize-with-pytestmarkparametrize) via `@pytest.mark.parametrize`.\n\n#### Executando Pytest\nPodemos executar o teste da linha de comando. Definimos o c√≥digo acima em um arquivo python. Da raiz do projeto, execute:\n\n`! LANGSMITH_TEST_SUITE='Email assistant: Test Tools For Interrupt'  pytest notebooks/test_tools.py`"
  },
  {
   "cell_type": "markdown",
   "id": "53165e98",
   "metadata": {},
   "source": "#### Visualizando Resultado do Experimento\n\nPodemos visualizar os resultados na interface do LangSmith. O `assert len(missing_calls) == 0` √© registrado na coluna `Pass` no LangSmith. Os `log_outputs` s√£o passados para a coluna `Outputs` e os argumentos da fun√ß√£o s√£o passados para a coluna `Inputs`. Cada entrada passada em `@pytest.mark.parametrize(` √© uma linha separada registrada no nome do projeto `LANGSMITH_TEST_SUITE` no LangSmith, que √© encontrado em `Datasets & Experiments`.\n\n![Test Results](img/test_result.png)"
  },
  {
   "cell_type": "markdown",
   "id": "fd325e27",
   "metadata": {},
   "source": "## Exemplo de Datasets do LangSmith\n\n![overview-img](img/eval_detail.png)\n\nVamos ver como podemos executar avalia√ß√µes com datasets do LangSmith. No exemplo anterior com Pytest, avaliamos a precis√£o de chamada de ferramenta do assistente de email. Agora, o dataset que vamos avaliar aqui √© especificamente para a etapa de triagem do assistente de email, classificando se um email requer uma resposta.\n\n#### Defini√ß√£o do Dataset\n\nPodemos [criar um dataset no LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#create-a-dataset) com o SDK do LangSmith. O c√≥digo abaixo cria um dataset com os casos de teste no arquivo `eval/email_dataset.py`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea997ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "from email_assistant.eval.email_dataset import examples_triage\n",
    "\n",
    "# Initialize LangSmith client\n",
    "client = Client()\n",
    "\n",
    "# Dataset name\n",
    "dataset_name = \"E-mail Triage Evaluation\"\n",
    "\n",
    "# Create dataset if it doesn't exist\n",
    "if not client.has_dataset(dataset_name=dataset_name):\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=\"A dataset of e-mails and their triage decisions.\"\n",
    "    )\n",
    "    # Add examples to the dataset\n",
    "    client.create_examples(dataset_id=dataset.id, examples=examples_triage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2df606",
   "metadata": {},
   "source": "#### Fun√ß√£o Alvo\n\nO dataset tem a seguinte estrutura, com uma entrada de e-mail e uma classifica√ß√£o de triagem verdadeira para o e-mail como sa√≠da:\n\n```\nexamples_triage = [\n  {\n      \"inputs\": {\"email_input\": email_input_1},\n      \"outputs\": {\"classification\": triage_output_1},   # NOTA: Isso se torna reference_output no dataset criado\n  }, ...\n]\n```"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7d7e83f-3006-4386-9230-786545c7b1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Example Input (inputs): {'email_input': {'author': 'Alice Smith <alice.smith@company.com>', 'to': 'Lance Martin <lance@company.com>', 'subject': 'Quick question about API documentation', 'email_thread': \"Hi Lance,\\n\\nI was reviewing the API documentation for the new authentication service and noticed a few endpoints seem to be missing from the specs. Could you help clarify if this was intentional or if we should update the docs?\\n\\nSpecifically, I'm looking at:\\n- /auth/refresh\\n- /auth/validate\\n\\nThanks!\\nAlice\"}}\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Example Input (inputs):\", examples_triage[0]['inputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f292f070-7af6-4370-9338-e90bfd6b3d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Example Reference Output (reference_outputs): {'classification': 'respond'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Example Reference Output (reference_outputs):\", examples_triage[0]['outputs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8290e820",
   "metadata": {},
   "source": "Definimos uma fun√ß√£o que recebe as entradas do dataset e as passa para nosso assistente de email. A [API de avalia√ß√£o](https://docs.smith.langchain.com/evaluation) do LangSmith passa o dicion√°rio `inputs` para esta fun√ß√£o. Esta fun√ß√£o ent√£o retorna um dicion√°rio com a sa√≠da do agente. Como estamos avaliando a etapa de triagem, precisamos apenas retornar a decis√£o de classifica√ß√£o."
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b9d1ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_email_assistant(inputs: dict) -> dict:\n",
    "    \"\"\"Process an email through the workflow-based email assistant.\"\"\"\n",
    "    response = email_assistant.nodes['triage_router'].invoke({\"email_input\": inputs[\"email_input\"]})\n",
    "    return {\"classification_decision\": response.update['classification_decision']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba6ec4c",
   "metadata": {},
   "source": "#### Fun√ß√£o Avaliadora\n\nAgora, criamos uma fun√ß√£o avaliadora. O que queremos avaliar? Temos sa√≠das de refer√™ncia em nosso dataset e sa√≠das do agente definidas nas fun√ß√µes acima.\n\n* Sa√≠das de refer√™ncia: `\"reference_outputs\": {\"classification\": triage_output_1} ...`\n* Sa√≠das do agente: `\"outputs\": {\"classification_decision\": agent_output_1} ...`\n\nQueremos avaliar se a sa√≠da do agente corresponde √† sa√≠da de refer√™ncia. Ent√£o, simplesmente precisamos de uma fun√ß√£o avaliadora que compare as duas, onde `outputs` √© a sa√≠da do agente e `reference_outputs` √© a sa√≠da de refer√™ncia do dataset."
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fee7532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_evaluator(outputs: dict, reference_outputs: dict) -> bool:\n",
    "    \"\"\"Check if the answer exactly matches the expected answer.\"\"\"\n",
    "    return outputs[\"classification_decision\"].lower() == reference_outputs[\"classification\"].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fd2de9",
   "metadata": {},
   "source": "### Executando Avalia√ß√£o\n\nAgora, a pergunta √©: como essas coisas se conectam? A API de avalia√ß√£o cuida disso para n√≥s. Ela passa o dicion√°rio `inputs` do nosso dataset para a fun√ß√£o alvo. Ela passa o dicion√°rio `reference_outputs` do nosso dataset para a fun√ß√£o avaliadora. E ela passa os `outputs` do nosso agente para a fun√ß√£o avaliadora.\n\nNote que isso √© similar ao que fizemos com Pytest: no Pytest, passamos as entradas e sa√≠das de refer√™ncia do exemplo do dataset para a fun√ß√£o de teste com `@pytest.mark.parametrize`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6807306d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'E-mail assistant workflow-484e19ad' at:\n",
      "https://smith.langchain.com/o/4fd66b18-e986-416b-9535-4f58f64dfdfa/datasets/53fec451-6ccd-4276-ab27-c69c45e93fc9/compare?selectedSessions=4479f261-b470-4477-82b8-03980099a65c\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b09dd10c2e44d179adaa2006c96206c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîî Classification: NOTIFY - This email contains important information\n",
      "üìß Classification: RESPOND - This email requires a response\n",
      "üìß Classification: RESPOND - This email requires a response\n",
      "üîî Classification: NOTIFY - This email contains important information\n",
      "üîî Classification: NOTIFY - This email contains important information\n",
      "üö´ Classification: IGNORE - This email can be safely ignored\n",
      "üìß Classification: RESPOND - This email requires a response\n",
      "üîî Classification: NOTIFY - This email contains important information\n",
      "üîî Classification: NOTIFY - This email contains important information\n",
      "üìß Classification: RESPOND - This email requires a response\n",
      "üö´ Classification: IGNORE - This email can be safely ignored\n",
      "üìß Classification: RESPOND - This email requires a response\n",
      "üìß Classification: RESPOND - This email requires a response\n",
      "üö´ Classification: IGNORE - This email can be safely ignored\n",
      "üìß Classification: RESPOND - This email requires a response\n",
      "üö´ Classification: IGNORE - This email can be safely ignored\n"
     ]
    }
   ],
   "source": [
    "# Set to true if you want to kick off evaluation\n",
    "run_expt = True\n",
    "if run_expt:\n",
    "    experiment_results_workflow = client.evaluate(\n",
    "        # Run agent\n",
    "        target_email_assistant,\n",
    "        # Dataset name\n",
    "        data=dataset_name,\n",
    "        # Evaluator\n",
    "        evaluators=[classification_evaluator],\n",
    "        # Name of the experiment\n",
    "        experiment_prefix=\"E-mail assistant workflow\",\n",
    "        # Number of concurrent evaluations\n",
    "        max_concurrency=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76baff88",
   "metadata": {},
   "source": "Podemos visualizar os resultados de ambos os experimentos na interface do LangSmith.\n\n![Test Results](img/eval.png)"
  },
  {
   "cell_type": "markdown",
   "id": "c5146b52",
   "metadata": {},
   "source": "## Avalia√ß√£o LLM-como-Juiz\n\nMostramos testes unit√°rios para a etapa de triagem (usando evaluate()) e chamada de ferramenta (usando Pytest).\n\nVamos demonstrar como voc√™ poderia usar um LLM como juiz para avaliar a execu√ß√£o do nosso agente contra um conjunto de crit√©rios de sucesso.\n\n![types](img/eval_types.png)\n\nPrimeiro, definimos um schema de sa√≠da estruturada para nosso avaliador LLM que cont√©m uma nota e justificativa para a nota."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d342b8",
   "metadata": {},
   "outputs": [],
   "source": "from pydantic import BaseModel, Field\nfrom langchain.chat_models import init_chat_model\n\nclass CriteriaGrade(BaseModel):\n    \"\"\"Pontuar a resposta contra crit√©rios espec√≠ficos.\"\"\"\n    justification: str = Field(description=\"A justificativa para a nota e pontua√ß√£o, incluindo exemplos espec√≠ficos da resposta.\")\n    grade: bool = Field(description=\"A resposta atende aos crit√©rios fornecidos?\")\n\n# Criar um LLM global para avalia√ß√£o para evitar recriar para cada teste\ncriteria_eval_llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google-genai\")\ncriteria_eval_structured_llm = criteria_eval_llm.with_structured_output(CriteriaGrade)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec02b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_input = email_inputs[0]\n",
    "print(\"Email Input:\", email_input)\n",
    "success_criteria = response_criteria_list[0]\n",
    "print(\"Success Criteria:\", success_criteria)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38390ccd",
   "metadata": {},
   "source": "Nosso Assistente de Email √© invocado com a entrada de email e a resposta √© formatada em uma string. Essas s√£o ent√£o passadas para o avaliador LLM para receber uma nota e justificativa para a nota."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbff28fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = email_assistant.invoke({\"email_input\": email_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64619fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from email_assistant.eval.prompts import RESPONSE_CRITERIA_SYSTEM_PROMPT\n",
    "\n",
    "all_messages_str = format_messages_string(response['messages'])\n",
    "eval_result = criteria_eval_structured_llm.invoke([\n",
    "        {\"role\": \"system\",\n",
    "            \"content\": RESPONSE_CRITERIA_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",\n",
    "            \"content\": f\"\"\"\\n\\n Response criteria: {success_criteria} \\n\\n Assistant's response: \\n\\n {all_messages_str} \\n\\n Evaluate whether the assistant's response meets the criteria and provide justification for your evaluation.\"\"\"}\n",
    "    ])\n",
    "\n",
    "eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64275647-6fdb-4bf3-806b-4dbc770cbd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE_CRITERIA_SYSTEM_PROMPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7994952c",
   "metadata": {},
   "source": "Podemos ver que o avaliador LLM retorna um resultado de avalia√ß√£o com um schema correspondente ao nosso modelo base `CriteriaGrade`."
  },
  {
   "cell_type": "markdown",
   "id": "0b44111d",
   "metadata": {},
   "source": "## Executando contra uma Su√≠te de Testes Maior\nAgora que vimos como avaliar nosso agente usando Pytest e evaluate(), e vimos um exemplo de usar um LLM como juiz, podemos usar avalia√ß√µes sobre uma su√≠te de testes maior para ter uma melhor no√ß√£o de como nosso agente se comporta em uma variedade mais ampla de exemplos."
  },
  {
   "cell_type": "markdown",
   "id": "9280d5ae-3070-4131-8763-454073176081",
   "metadata": {},
   "source": [
    "Let's run our email_assistant against a larger test suite.\n",
    "```\n",
    "! LANGSMITH_TEST_SUITE='Email assistant: Test Full Response Interrupt' LANGSMITH_EXPERIMENT='email_assistant' pytest tests/test_response.py --agent-module email_assistant\n",
    "```\n",
    "\n",
    "In `test_response.py`, you can see a few things. \n",
    "\n",
    "We pass our dataset examples into functions that will run pytest and log to our `LANGSMITH_TEST_SUITE`:\n",
    "\n",
    "```\n",
    "# Reference output key\n",
    "@pytest.mark.langsmith(output_keys=[\"criteria\"])\n",
    "# Variable names and a list of tuples with the test cases\n",
    "# Each test case is (email_input, email_name, criteria, expected_calls)\n",
    "@pytest.mark.parametrize(\"email_input,email_name,criteria,expected_calls\",create_response_test_cases())\n",
    "def test_response_criteria_evaluation(email_input, email_name, criteria, expected_calls):\n",
    "```\n",
    "\n",
    "We use LLM-as-judge with a grading schema:\n",
    "```\n",
    "class CriteriaGrade(BaseModel):\n",
    "    \"\"\"Score the response against specific criteria.\"\"\"\n",
    "    grade: bool = Field(description=\"Does the response meet the provided criteria?\")\n",
    "    justification: str = Field(description=\"The justification for the grade and score, including specific examples from the response.\")\n",
    "```\n",
    "\n",
    "\n",
    "We evaluate the agent response relative to the criteria:\n",
    "```\n",
    "    # Evaluate against criteria\n",
    "    eval_result = criteria_eval_structured_llm.invoke([\n",
    "        {\"role\": \"system\",\n",
    "            \"content\": RESPONSE_CRITERIA_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",\n",
    "            \"content\": f\"\"\"\\n\\n Response criteria: {criteria} \\n\\n Assistant's response: \\n\\n {all_messages_str} \\n\\n Evaluate whether the assistant's response meets the criteria and provide justification for your evaluation.\"\"\"}\n",
    "    ])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca836fbf",
   "metadata": {},
   "source": "Agora vamos dar uma olhada neste experimento na interface do LangSmith e ver em que nosso agente foi bom, e o que poderia melhorar.\n\n#### Obtendo Resultados\n\nTamb√©m podemos obter os resultados da avalia√ß√£o lendo o projeto de rastreamento associado ao nosso experimento. Isso √© √≥timo para criar visualiza√ß√µes customizadas do desempenho do nosso agente."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b655f8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# TODO: Copy your experiment name here\n",
    "experiment_name = \"email_assistant:8286b3b8\"\n",
    "# Set this to load expt results\n",
    "load_expt = False\n",
    "if load_expt:\n",
    "    email_assistant_experiment_results = client.read_project(project_name=experiment_name, include_stats=True)\n",
    "    print(\"Latency p50:\", email_assistant_experiment_results.latency_p50)\n",
    "    print(\"Latency p99:\", email_assistant_experiment_results.latency_p99)\n",
    "    print(\"Token Usage:\", email_assistant_experiment_results.total_tokens)\n",
    "    print(\"Feedback Stats:\", email_assistant_experiment_results.feedback_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccdfaa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}