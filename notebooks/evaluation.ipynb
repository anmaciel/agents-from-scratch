{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68e21aa1",
   "metadata": {},
   "source": [
    "# Avaliação de Agentes\n",
    "\n",
    "Temos um assistente de email que usa um roteador para triagem de emails e então passa o email para o agente gerar uma resposta. Como podemos ter certeza de que funcionará bem em produção? É por isso que os testes são importantes: eles orientam nossas decisões sobre a arquitetura do agente com métricas quantificáveis como qualidade da resposta, uso de tokens, latência ou precisão da triagem. O [LangSmith](https://docs.smith.langchain.com/) oferece duas maneiras principais de testar agentes.\n",
    "\n",
    "![overview-img](img/overview_eval.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7f7048",
   "metadata": {},
   "source": [
    "#### Carregar Variáveis de Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47d4c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2005c34d",
   "metadata": {},
   "source": [
    "## Como Executar Avaliações\n",
    "\n",
    "#### Pytest / Vitest\n",
    "\n",
    "[Pytest](https://docs.pytest.org/en/stable/) e Vitest são bem conhecidos por muitos desenvolvedores como ferramentas poderosas para escrever testes dentro dos ecossistemas Python e JavaScript. O LangSmith se integra com esses frameworks para permitir que você escreva e execute testes que registram resultados no LangSmith. Para este notebook, usaremos Pytest.\n",
    "* Pytest é uma ótima maneira de começar para desenvolvedores que já estão familiarizados com o framework.\n",
    "* Pytest é ótimo para avaliações mais complexas, onde cada caso de teste do agente requer verificações específicas e critérios de sucesso que são mais difíceis de generalizar.\n",
    "\n",
    "#### Datasets do LangSmith\n",
    "\n",
    "Você também pode criar um dataset [no LangSmith](https://docs.smith.langchain.com/evaluation) e executar nosso assistente contra o dataset usando a API de avaliação do LangSmith.\n",
    "* Os datasets do LangSmith são ótimos para equipes que estão construindo colaborativamente sua suíte de testes.\n",
    "* Você pode aproveitar traces de produção, filas de anotação, geração de dados sintéticos, e mais, para adicionar exemplos a um dataset dourado sempre crescente.\n",
    "* Os datasets do LangSmith são ótimos quando você pode definir avaliadores que podem ser aplicados a todos os casos de teste no dataset (ex. similaridade, precisão de correspondência exata, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b7c989",
   "metadata": {},
   "source": [
    "## Casos de Teste\n",
    "\n",
    "Os testes geralmente começam com a definição dos casos de teste, o que pode ser um processo desafiador. Neste caso, vamos apenas definir um conjunto de emails de exemplo que queremos gerenciar junto com algumas coisas para testar. Você pode ver os casos de teste em `eval/email_dataset.py`, que contém o seguinte:\n",
    "\n",
    "1. **Emails de Entrada**: Uma coleção de exemplos diversos de email\n",
    "2. **Classificações Verdadeiras**: `Respond`, `Notify`, `Ignore`\n",
    "3. **Chamadas de Ferramentas Esperadas**: Ferramentas chamadas para cada email que requer uma resposta\n",
    "4. **Critérios de Resposta**: O que torna uma boa resposta para emails que requerem respostas\n",
    "\n",
    "Note que temos tanto:\n",
    "- Testes \"de integração\" ponta a ponta (ex. Emails de Entrada -> Agente -> Saída Final vs Critérios de Resposta)\n",
    "- Testes para etapas específicas em nosso fluxo de trabalho (ex. Emails de Entrada -> Agente -> Classificação vs Classificação Verdadeira)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fdc2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from email_assistant.eval.email_dataset import email_inputs, expected_tool_calls, triage_outputs_list, response_criteria_list\n",
    "\n",
    "test_case_ix = 0\n",
    "\n",
    "print(\"Entrada de Email:\", email_inputs[test_case_ix])\n",
    "print(\"Saída de Triagem Esperada:\", triage_outputs_list[test_case_ix])\n",
    "print(\"Chamadas de Ferramenta Esperadas:\", expected_tool_calls[test_case_ix])\n",
    "print(\"Critérios de Resposta:\", response_criteria_list[test_case_ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2337bd7c",
   "metadata": {},
   "source": [
    "## Exemplo com Pytest\n",
    "\n",
    "Vamos ver como podemos escrever um teste para uma parte específica do nosso fluxo de trabalho com Pytest. Testaremos se nosso `email_assistant` faz as chamadas de ferramentas corretas ao responder aos emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae92fe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from email_assistant.eval.email_dataset import email_inputs, expected_tool_calls\n",
    "from email_assistant.utils import format_messages_string\n",
    "from email_assistant.email_assistant import email_assistant\n",
    "from email_assistant.utils import extract_tool_calls\n",
    "\n",
    "from langsmith import testing as t\n",
    "\n",
    "@pytest.mark.langsmith\n",
    "@pytest.mark.parametrize(\n",
    "    \"email_input, expected_calls\",\n",
    "    [   # Pick some examples with e-mail reply expected\n",
    "        (email_inputs[0],expected_tool_calls[0]),\n",
    "        (email_inputs[3],expected_tool_calls[3]),\n",
    "    ],\n",
    ")\n",
    "def test_email_dataset_tool_calls(email_input, expected_calls):\n",
    "    \"\"\"Test if email processing contains expected tool calls.\n",
    "\n",
    "    This test confirms that all expected tools are called during email processing,\n",
    "    but does not check the order of tool invocations or the number of invocations\n",
    "    per tool. Additional checks for these aspects could be added if desired.\n",
    "    \"\"\"\n",
    "    # Run the email assistant\n",
    "    messages = [{\"role\": \"user\", \"content\": str(email_input)}]\n",
    "    result = email_assistant.invoke({\"messages\": messages})\n",
    "\n",
    "    # Extract tool calls from messages list\n",
    "    extracted_tool_calls = extract_tool_calls(result['messages'])\n",
    "\n",
    "    # Check if all expected tool calls are in the extracted ones\n",
    "    missing_calls = [call for call in expected_calls if call.lower() not in extracted_tool_calls]\n",
    "\n",
    "    t.log_outputs({\n",
    "                \"missing_calls\": missing_calls,\n",
    "                \"extracted_tool_calls\": extracted_tool_calls,\n",
    "                \"response\": format_messages_string(result['messages'])\n",
    "            })\n",
    "\n",
    "    # Test passes if no expected calls are missing\n",
    "    assert len(missing_calls) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700aba2a",
   "metadata": {},
   "source": [
    "Você notará algumas coisas:\n",
    "- Para [executar com Pytest e registrar resultados de teste no LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest), precisamos apenas adicionar o decorador `@pytest.mark.langsmith` à nossa função e colocá-la em um arquivo, como você vê em `notebooks/test_tools.py`. Isso registrará os resultados do teste no LangSmith.\n",
    "- Segundo, podemos passar exemplos de dataset para a função de teste como mostrado [aqui](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#parametrize-with-pytestmarkparametrize) via `@pytest.mark.parametrize`.\n",
    "\n",
    "#### Executando Pytest\n",
    "Podemos executar o teste da linha de comando. Definimos o código acima em um arquivo python. Da raiz do projeto, execute:\n",
    "\n",
    "`! LANGSMITH_TEST_SUITE='Email assistant: Test Tools For Interrupt'  pytest notebooks/test_tools.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53165e98",
   "metadata": {},
   "source": [
    "#### Visualizando Resultado do Experimento\n",
    "\n",
    "Podemos visualizar os resultados na interface do LangSmith. O `assert len(missing_calls) == 0` é registrado na coluna `Pass` no LangSmith. Os `log_outputs` são passados para a coluna `Outputs` e os argumentos da função são passados para a coluna `Inputs`. Cada entrada passada em `@pytest.mark.parametrize(` é uma linha separada registrada no nome do projeto `LANGSMITH_TEST_SUITE` no LangSmith, que é encontrado em `Datasets & Experiments`.\n",
    "\n",
    "![Test Results](img/test_result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd325e27",
   "metadata": {},
   "source": [
    "## Exemplo de Datasets do LangSmith\n",
    "\n",
    "![overview-img](img/eval_detail.png)\n",
    "\n",
    "Vamos ver como podemos executar avaliações com datasets do LangSmith. No exemplo anterior com Pytest, avaliamos a precisão de chamada de ferramenta do assistente de email. Agora, o dataset que vamos avaliar aqui é especificamente para a etapa de triagem do assistente de email, classificando se um email requer uma resposta.\n",
    "\n",
    "#### Definição do Dataset\n",
    "\n",
    "Podemos [criar um dataset no LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#create-a-dataset) com o SDK do LangSmith. O código abaixo cria um dataset com os casos de teste no arquivo `eval/email_dataset.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea997ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "from email_assistant.eval.email_dataset import examples_triage\n",
    "\n",
    "# Initialize LangSmith client\n",
    "client = Client()\n",
    "\n",
    "# Dataset name\n",
    "dataset_name = \"E-mail Triage Evaluation\"\n",
    "\n",
    "# Create dataset if it doesn't exist\n",
    "if not client.has_dataset(dataset_name=dataset_name):\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=\"A dataset of e-mails and their triage decisions.\"\n",
    "    )\n",
    "    # Add examples to the dataset\n",
    "    client.create_examples(dataset_id=dataset.id, examples=examples_triage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2df606",
   "metadata": {},
   "source": [
    "#### Função Alvo\n",
    "\n",
    "O dataset tem a seguinte estrutura, com uma entrada de e-mail e uma classificação de triagem verdadeira para o e-mail como saída:\n",
    "\n",
    "```\n",
    "examples_triage = [\n",
    "  {\n",
    "      \"inputs\": {\"email_input\": email_input_1},\n",
    "      \"outputs\": {\"classification\": triage_output_1},   # NOTA: Isso se torna reference_output no dataset criado\n",
    "  }, ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d7e83f-3006-4386-9230-786545c7b1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Entrada de Exemplo do Dataset (inputs):\", examples_triage[0]['inputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f292f070-7af6-4370-9338-e90bfd6b3d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saída de Referência de Exemplo do Dataset (reference_outputs):\", examples_triage[0]['outputs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8290e820",
   "metadata": {},
   "source": [
    "Definimos uma função que recebe as entradas do dataset e as passa para nosso assistente de email. A [API de avaliação](https://docs.smith.langchain.com/evaluation) do LangSmith passa o dicionário `inputs` para esta função. Esta função então retorna um dicionário com a saída do agente. Como estamos avaliando a etapa de triagem, precisamos apenas retornar a decisão de classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9d1ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_email_assistant(inputs: dict) -> dict:\n",
    "    \"\"\"Process an email through the workflow-based email assistant.\"\"\"\n",
    "    response = email_assistant.nodes['triage_router'].invoke({\"email_input\": inputs[\"email_input\"]})\n",
    "    return {\"classification_decision\": response.update['classification_decision']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba6ec4c",
   "metadata": {},
   "source": [
    "#### Função Avaliadora\n",
    "\n",
    "Agora, criamos uma função avaliadora. O que queremos avaliar? Temos saídas de referência em nosso dataset e saídas do agente definidas nas funções acima.\n",
    "\n",
    "* Saídas de referência: `\"reference_outputs\": {\"classification\": triage_output_1} ...`\n",
    "* Saídas do agente: `\"outputs\": {\"classification_decision\": agent_output_1} ...`\n",
    "\n",
    "Queremos avaliar se a saída do agente corresponde à saída de referência. Então, simplesmente precisamos de uma função avaliadora que compare as duas, onde `outputs` é a saída do agente e `reference_outputs` é a saída de referência do dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fee7532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_evaluator(outputs: dict, reference_outputs: dict) -> bool:\n",
    "    \"\"\"Check if the answer exactly matches the expected answer.\"\"\"\n",
    "    return outputs[\"classification_decision\"].lower() == reference_outputs[\"classification\"].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fd2de9",
   "metadata": {},
   "source": [
    "### Executando Avaliação\n",
    "\n",
    "Agora, a pergunta é: como essas coisas se conectam? A API de avaliação cuida disso para nós. Ela passa o dicionário `inputs` do nosso dataset para a função alvo. Ela passa o dicionário `reference_outputs` do nosso dataset para a função avaliadora. E ela passa os `outputs` do nosso agente para a função avaliadora.\n",
    "\n",
    "Note que isso é similar ao que fizemos com Pytest: no Pytest, passamos as entradas e saídas de referência do exemplo do dataset para a função de teste com `@pytest.mark.parametrize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6807306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to true if you want to kick off evaluation\n",
    "run_expt = True\n",
    "if run_expt:\n",
    "    experiment_results_workflow = client.evaluate(\n",
    "        # Run agent\n",
    "        target_email_assistant,\n",
    "        # Dataset name\n",
    "        data=dataset_name,\n",
    "        # Evaluator\n",
    "        evaluators=[classification_evaluator],\n",
    "        # Name of the experiment\n",
    "        experiment_prefix=\"E-mail assistant workflow\",\n",
    "        # Number of concurrent evaluations\n",
    "        max_concurrency=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76baff88",
   "metadata": {},
   "source": [
    "Podemos visualizar os resultados de ambos os experimentos na interface do LangSmith.\n",
    "\n",
    "![Test Results](img/eval.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5146b52",
   "metadata": {},
   "source": [
    "## Avaliação LLM-como-Juiz\n",
    "\n",
    "Mostramos testes unitários para a etapa de triagem (usando evaluate()) e chamada de ferramenta (usando Pytest).\n",
    "\n",
    "Vamos demonstrar como você poderia usar um LLM como juiz para avaliar a execução do nosso agente contra um conjunto de critérios de sucesso.\n",
    "\n",
    "![types](img/eval_types.png)\n",
    "\n",
    "Primeiro, definimos um schema de saída estruturada para nosso avaliador LLM que contém uma nota e justificativa para a nota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d342b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "class CriteriaGrade(BaseModel):\n",
    "    \"\"\"Pontuar a resposta contra critérios específicos.\"\"\"\n",
    "    justification: str = Field(description=\"A justificativa para a nota e pontuação, incluindo exemplos específicos da resposta.\")\n",
    "    grade: bool = Field(description=\"A resposta atende aos critérios fornecidos?\")\n",
    "\n",
    "# Criar um LLM global para avaliação para evitar recriar para cada teste\n",
    "criteria_eval_llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google-genai\")\n",
    "criteria_eval_structured_llm = criteria_eval_llm.with_structured_output(CriteriaGrade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec02b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_input = email_inputs[0]\n",
    "print(\"Entrada de Email:\", email_input)\n",
    "success_criteria = response_criteria_list[0]\n",
    "print(\"Critérios de Sucesso:\", success_criteria)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38390ccd",
   "metadata": {},
   "source": [
    "Nosso Assistente de Email é invocado com a entrada de email e a resposta é formatada em uma string. Essas são então passadas para o avaliador LLM para receber uma nota e justificativa para a nota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbff28fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = email_assistant.invoke({\"email_input\": email_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64619fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from email_assistant.eval.prompts import RESPONSE_CRITERIA_SYSTEM_PROMPT\n",
    "\n",
    "all_messages_str = format_messages_string(response['messages'])\n",
    "eval_result = criteria_eval_structured_llm.invoke([\n",
    "        {\"role\": \"system\",\n",
    "            \"content\": RESPONSE_CRITERIA_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",\n",
    "            \"content\": f\"\"\"\\n\\n Response criteria: {success_criteria} \\n\\n Assistant's response: \\n\\n {all_messages_str} \\n\\n Evaluate whether the assistant's response meets the criteria and provide justification for your evaluation.\"\"\"}\n",
    "    ])\n",
    "\n",
    "eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64275647-6fdb-4bf3-806b-4dbc770cbd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE_CRITERIA_SYSTEM_PROMPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7994952c",
   "metadata": {},
   "source": [
    "Podemos ver que o avaliador LLM retorna um resultado de avaliação com um schema correspondente ao nosso modelo base `CriteriaGrade`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b44111d",
   "metadata": {},
   "source": [
    "## Executando contra uma Suíte de Testes Maior\n",
    "Agora que vimos como avaliar nosso agente usando Pytest e evaluate(), e vimos um exemplo de usar um LLM como juiz, podemos usar avaliações sobre uma suíte de testes maior para ter uma melhor noção de como nosso agente se comporta em uma variedade mais ampla de exemplos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9280d5ae-3070-4131-8763-454073176081",
   "metadata": {},
   "source": [
    "Let's run our email_assistant against a larger test suite.\n",
    "```\n",
    "! LANGSMITH_TEST_SUITE='Email assistant: Test Full Response Interrupt' LANGSMITH_EXPERIMENT='email_assistant' pytest tests/test_response.py --agent-module email_assistant\n",
    "```\n",
    "\n",
    "In `test_response.py`, you can see a few things. \n",
    "\n",
    "We pass our dataset examples into functions that will run pytest and log to our `LANGSMITH_TEST_SUITE`:\n",
    "\n",
    "```\n",
    "# Reference output key\n",
    "@pytest.mark.langsmith(output_keys=[\"criteria\"])\n",
    "# Variable names and a list of tuples with the test cases\n",
    "# Each test case is (email_input, email_name, criteria, expected_calls)\n",
    "@pytest.mark.parametrize(\"email_input,email_name,criteria,expected_calls\",create_response_test_cases())\n",
    "def test_response_criteria_evaluation(email_input, email_name, criteria, expected_calls):\n",
    "```\n",
    "\n",
    "We use LLM-as-judge with a grading schema:\n",
    "```\n",
    "class CriteriaGrade(BaseModel):\n",
    "    \"\"\"Score the response against specific criteria.\"\"\"\n",
    "    grade: bool = Field(description=\"Does the response meet the provided criteria?\")\n",
    "    justification: str = Field(description=\"The justification for the grade and score, including specific examples from the response.\")\n",
    "```\n",
    "\n",
    "\n",
    "We evaluate the agent response relative to the criteria:\n",
    "```\n",
    "    # Evaluate against criteria\n",
    "    eval_result = criteria_eval_structured_llm.invoke([\n",
    "        {\"role\": \"system\",\n",
    "            \"content\": RESPONSE_CRITERIA_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",\n",
    "            \"content\": f\"\"\"\\n\\n Response criteria: {criteria} \\n\\n Assistant's response: \\n\\n {all_messages_str} \\n\\n Evaluate whether the assistant's response meets the criteria and provide justification for your evaluation.\"\"\"}\n",
    "    ])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca836fbf",
   "metadata": {},
   "source": [
    "Agora vamos dar uma olhada neste experimento na interface do LangSmith e ver em que nosso agente foi bom, e o que poderia melhorar.\n",
    "\n",
    "#### Obtendo Resultados\n",
    "\n",
    "Também podemos obter os resultados da avaliação lendo o projeto de rastreamento associado ao nosso experimento. Isso é ótimo para criar visualizações customizadas do desempenho do nosso agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b655f8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# TODO: Copy your experiment name here\n",
    "experiment_name = \"email_assistant:8286b3b8\"\n",
    "# Set this to load expt results\n",
    "load_expt = False\n",
    "if load_expt:\n",
    "    email_assistant_experiment_results = client.read_project(project_name=experiment_name, include_stats=True)\n",
    "    print(\"Latência p50:\", email_assistant_experiment_results.latency_p50)\n",
    "    print(\"Latência p99:\", email_assistant_experiment_results.latency_p99)\n",
    "    print(\"Uso de Tokens:\", email_assistant_experiment_results.total_tokens)\n",
    "    print(\"Estatísticas de Feedback:\", email_assistant_experiment_results.feedback_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccdfaa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
