{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc082433",
   "metadata": {},
   "source": [
    "# LangGraph 101\n",
    "\n",
    "[LLMs](https://python.langchain.com/docs/concepts/chat_models/) tornam possível incorporar inteligência em uma nova classe de aplicações. [LangGraph](https://langchain-ai.github.io/langgraph/) é um framework para ajudar a construir aplicações com LLMs. Aqui, vamos revisar o básico do LangGraph, explicar seus benefícios, mostrar como usá-lo para construir fluxos de trabalho / agentes, e mostrar como funciona com [LangChain](https://www.langchain.com/) / [LangSmith](https://docs.smith.langchain.com/).\n",
    "\n",
    "![ecosystem](./img/ecosystem.png)\n",
    "\n",
    "## Modelos de chat\n",
    "\n",
    "[Modelos de chat](https://python.langchain.com/docs/concepts/chat_models/) são a base das aplicações LLM. Eles são tipicamente acessados através de uma interface de chat que recebe uma lista de [mensagens](https://python.langchain.com/docs/concepts/messages/) como entrada e retorna uma [mensagem](https://python.langchain.com/docs/concepts/messages/) como saída. LangChain fornece [uma interface padronizada para modelos de chat](https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html), facilitando [acessar muitos provedores diferentes](https://python.langchain.com/docs/integrations/chat/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecc2b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ee8f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google-genai\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50777b0b",
   "metadata": {},
   "source": [
    "## Executando o modelo\n",
    "\n",
    "A interface `init_chat_model` fornece métodos [padronizados](https://python.langchain.com/docs/concepts/runnables/) para usar modelos de chat, que incluem:\n",
    "- `invoke()`: Uma única entrada é transformada em uma saída.\n",
    "- `stream()`: As saídas são [transmitidas](https://python.langchain.com/docs/concepts/streaming/#stream-and-astream) conforme são produzidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28159d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke(\"O que é um agente?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41137023",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1d00eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.markdown import Markdown\n",
    "Markdown(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a24d8ef",
   "metadata": {},
   "source": [
    "## Ferramentas\n",
    "\n",
    "[Ferramentas](https://python.langchain.com/docs/concepts/tools/) são utilitários que podem ser chamados por um modelo de chat. No LangChain, criar ferramentas pode ser feito usando o decorador `@tool`, que transforma funções Python em ferramentas chamáveis. Ele automaticamente inferirá o nome da ferramenta, descrição e argumentos esperados da definição da função. Você também pode usar [servidores Model Context Protocol (MCP)](https://github.com/langchain-ai/langchain-mcp-adapters) como ferramentas compatíveis com LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdff275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def write_email(to: str, subject: str, content: str) -> str:\n",
    "    \"\"\"Escrever e enviar um email.\"\"\"\n",
    "    # Resposta placeholder - em aplicação real enviaria email\n",
    "    return f\"Email enviado para {to} com assunto '{subject}' e conteúdo: {content}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52ec55b-0b60-4b0c-95d4-ff528a64694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(write_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a40647-3d48-4760-aabe-144d627de110",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_email.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd85ae4-9d4c-4efa-9577-aca96e9f22cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(write_email.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a6b427",
   "metadata": {},
   "source": [
    "## Chamada de Ferramentas\n",
    "\n",
    "Ferramentas podem ser [chamadas](https://python.langchain.com/docs/concepts/tool_calling/) por LLMs. Quando uma ferramenta é vinculada ao modelo, o modelo pode escolher chamar a ferramenta retornando uma saída estruturada com argumentos da ferramenta. Usamos o método `bind_tools` para aumentar um LLM com ferramentas.\n",
    "\n",
    "![tool-img](img/tool_call_detail.png)\n",
    "\n",
    "Provedores frequentemente têm [parâmetros como `tool_choice`](https://python.langchain.com/docs/how_to/tool_choice/) para forçar a chamada de ferramentas específicas. `any` selecionará pelo menos uma das ferramentas.\n",
    "\n",
    "Além disso, podemos [definir `parallel_tool_calls=False`](https://python.langchain.com/docs/how_to/tool_calling_parallel/) para garantir que o modelo só chamará uma ferramenta por vez (funcionalidade suportada apenas pela OpenAI e Anthropic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa57bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conectar ferramentas a um modelo de chat\n",
    "model_with_tools = llm.bind_tools([write_email])\n",
    "\n",
    "# O modelo agora será capaz de chamar ferramentas\n",
    "output = model_with_tools.invoke(\"Rascunhe uma resposta para meu chefe (chefe@empresa.com.br) sobre a reunião de amanhã\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7985eab6-9e6b-4fa5-8027-52d32886b97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0ce030-e760-4679-838f-d88d1480664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717779cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrair chamadas de ferramenta e executá-las\n",
    "if output.tool_calls:\n",
    "\targs = output.tool_calls[0]['args']\n",
    "\targs\n",
    "else:\n",
    "\tprint(\"Nenhuma chamada de ferramenta encontrada na resposta do modelo.\")\n",
    "\targs = None\n",
    "\targs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f85694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrair os argumentos da chamada de ferramenta do output\n",
    "if hasattr(output, \"tool_calls\") and output.tool_calls:\n",
    "\targs = output.tool_calls[0]['args']\n",
    "\tresult = write_email.invoke(args)\n",
    "\tMarkdown(result)\n",
    "else:\n",
    "\tprint(\"Nenhuma chamada de ferramenta encontrada na resposta do modelo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f9c52a",
   "metadata": {},
   "source": [
    "![basic_prompt](img/tool_call.png)\n",
    "\n",
    "## Fluxos de Trabalho\n",
    " \n",
    "Existem muitos padrões para construir aplicações com LLMs.\n",
    "\n",
    "[Podemos incorporar chamadas de LLM em fluxos de trabalho pré-definidos](https://langchain-ai.github.io/langgraph/tutorials/workflows/), dando ao sistema mais autonomia para tomar decisões.\n",
    "\n",
    "Como exemplo, poderíamos adicionar uma etapa de roteador para determinar se devemos escrever um email ou não.\n",
    "\n",
    "![workflow_example](img/workflow_example.png)\n",
    "\n",
    "## Agentes\n",
    "\n",
    "Podemos aumentar ainda mais a autonomia, permitindo que o LLM direcione dinamicamente seu próprio uso de ferramentas.\n",
    "\n",
    "[Agentes](https://langchain-ai.github.io/langgraph/tutorials/workflows/#agent) são tipicamente implementados como chamada de ferramenta em um loop, onde a saída de cada chamada de ferramenta é usada para informar a próxima ação.\n",
    "\n",
    "![agent_example](img/agent_example.png)\n",
    "\n",
    "Agentes são bem adequados para problemas abertos onde é difícil prever os passos *exatos* necessários com antecedência.\n",
    "\n",
    "Fluxos de trabalho são frequentemente apropriados quando o fluxo de controle pode ser facilmente definido com antecedência.\n",
    "\n",
    "![workflow_v_agent](img/workflow_v_agent.png)\n",
    "\n",
    "## O que é LangGraph?\n",
    "\n",
    "[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) fornece infraestrutura de suporte de baixo nível que fica por baixo de *qualquer* fluxo de trabalho ou agente.\n",
    "\n",
    "Ele não abstrai prompts ou arquitetura, e fornece alguns benefícios:\n",
    "\n",
    "- **Controle**: Facilita definir e/ou combinar agentes e fluxos de trabalho.\n",
    "- **Persistência**: Fornece uma maneira de persistir o estado de um grafo, o que permite tanto memória quanto humano-no-loop.\n",
    "- **Teste, Depuração e Deployment**: Fornece uma rampa fácil para testar, depurar e fazer deploy de aplicações.\n",
    "\n",
    "### Controle\n",
    "\n",
    "LangGraph permite definir sua aplicação como um grafo com:\n",
    "\n",
    "1. *Estado*: Que informações precisamos rastrear ao longo da aplicação?\n",
    "2. *Nós*: Como queremos atualizar essas informações ao longo da aplicação?\n",
    "3. *Arestas*: Como queremos conectar esses nós?\n",
    "\n",
    "Podemos usar a classe [`StateGraph`](https://langchain-ai.github.io/langgraph/concepts/low_level/#graphs) para inicializar um grafo LangGraph com um objeto [`State`](https://langchain-ai.github.io/langgraph/concepts/low_level/#state).\n",
    "\n",
    "`State` define o esquema para as informações que queremos rastrear ao longo da aplicação.\n",
    "\n",
    "Isso pode ser qualquer objeto com `getattr()` em python, como um dicionário, dataclass, ou objeto Pydantic:\n",
    "\n",
    "- TypeDict é mais rápido mas não suporta padrões\n",
    "- Dataclass é basicamente tão rápido, suporta sintaxe de ponto `state.foo`, e tem padrões.\n",
    "- Pydantic é mais lento (especialmente com validadores customizados) mas fornece validação de tipo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3319290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "class StateSchema(TypedDict):\n",
    "    request: str\n",
    "    email: str\n",
    "\n",
    "workflow = StateGraph(StateSchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84bedb9",
   "metadata": {},
   "source": [
    "Cada nó é simplesmente uma função python ou código typescript. Isso nos dá controle total sobre a lógica dentro de cada nó.\n",
    "\n",
    "Eles recebem o estado atual, e retornam um dicionário para atualizar o estado.\n",
    "\n",
    "Por padrão, [chaves de estado são sobrescritas](https://langchain-ai.github.io/langgraph/how-tos/state-reducers/).\n",
    "\n",
    "No entanto, você pode [definir lógica de atualização customizada](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers).\n",
    "\n",
    "![nodes_edges](img/nodes_edges.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e79c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_email_node(state: StateSchema) -> StateSchema:\n",
    "    # Código imperativo que processa a solicitação\n",
    "    output = model_with_tools.invoke(state[\"request\"])\n",
    "    args = output.tool_calls[0]['args']\n",
    "    email = write_email.invoke(args)\n",
    "    return {\"email\": email}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737c8040",
   "metadata": {},
   "source": [
    "Arestas conectam nós.\n",
    "\n",
    "Especificamos o fluxo de controle adicionando arestas e nós ao nosso grafo de estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554e0d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(StateSchema)\n",
    "workflow.add_node(\"write_email_node\", write_email_node)\n",
    "workflow.add_edge(START, \"write_email_node\")\n",
    "workflow.add_edge(\"write_email_node\", END)\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc79b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.invoke({\"request\": \"Rascunhe uma resposta para meu chefe (chefe@empresa.com.br) sobre a reunião de amanhã\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2446dea9",
   "metadata": {},
   "source": [
    "O roteamento entre nós pode ser feito [condicionalmente](https://langchain-ai.github.io/langgraph/concepts/low_level/#conditional-edges) usando uma função simples.\n",
    "\n",
    "O valor de retorno desta função é usado como o nome do nó (ou lista de nós) para o qual enviar o estado a seguir.\n",
    "\n",
    "Você pode opcionalmente fornecer um dicionário que mapeia a saída `should_continue` para o nome do próximo nó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29b05bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langgraph.graph import MessagesState\n",
    "from email_assistant.utils import show_graph\n",
    "\n",
    "def call_llm(state: MessagesState) -> MessagesState:\n",
    "    \"\"\"Run LLM\"\"\"\n",
    "\n",
    "    output = model_with_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [output]}\n",
    "\n",
    "def run_tool(state: MessagesState):\n",
    "    \"\"\"Performs the tool call\"\"\"\n",
    "\n",
    "    result = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        observation = write_email.invoke(tool_call[\"args\"])\n",
    "        result.append({\"role\": \"tool\", \"content\": observation, \"tool_call_id\": tool_call[\"id\"]})\n",
    "    return {\"messages\": result}\n",
    "\n",
    "def should_continue(state: MessagesState) -> Literal[\"run_tool\", \"__end__\"]:\n",
    "    \"\"\"Route to tool handler, or end if Done tool called\"\"\"\n",
    "\n",
    "    # Get the last message\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    # If the last message is a tool call, check if it's a Done tool call\n",
    "    if last_message.tool_calls:\n",
    "        return \"run_tool\"\n",
    "    # Otherwise, we stop (reply to the user)\n",
    "    return END\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"call_llm\", call_llm)\n",
    "workflow.add_node(\"run_tool\", run_tool)\n",
    "workflow.add_edge(START, \"call_llm\")\n",
    "workflow.add_conditional_edges(\"call_llm\", should_continue, {\"run_tool\": \"run_tool\", END: END})\n",
    "workflow.add_edge(\"run_tool\", END)\n",
    "\n",
    "# Run the workflow\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9f07af-c633-4527-b2d9-52c6451dc9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadbafde",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = app.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Draft a response to my boss (boss@company.ai) confirming that I want to attend Interrupt!\"}]})\n",
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78b232d",
   "metadata": {},
   "source": [
    "Com esses componentes de baixo nível, você pode construir muitos fluxos de trabalho e agentes diferentes. Veja [este tutorial](https://langchain-ai.github.io/langgraph/tutorials/workflows/)!\n",
    "\n",
    "Como agentes são um padrão tão comum, o [LangGraph](https://langchain-ai.github.io/langgraph/tutorials/workflows/#pre-built) tem [uma abstração de agente pré-construída](https://langchain-ai.github.io/langgraph/agents/overview/?ref=blog.langchain.dev#what-is-an-agent).\n",
    "\n",
    "Com o [método pré-construído](https://langchain-ai.github.io/langgraph/tutorials/workflows/#pre-built) do LangGraph, apenas passamos o LLM, ferramentas e prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a317ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[write_email],\n",
    "    prompt=\"Respond to the user's request using the tools provided.\"\n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "result = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Draft a response to my boss (boss@company.ai) confirming that I want to attend Interrupt!\"}]}\n",
    ")\n",
    "\n",
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6e506f",
   "metadata": {},
   "source": [
    "### Persistência\n",
    "\n",
    "#### Threads\n",
    "\n",
    "Pode ser muito útil permitir que agentes pausem durante tarefas de longa duração.\n",
    "\n",
    "LangGraph tem uma camada de persistência integrada, implementada através de checkpointers, para permitir isso.\n",
    "\n",
    "Quando você compila o grafo com um checkpointer, o checkpointer salva um [checkpoint](https://langchain-ai.github.io/langgraph/concepts/persistence/#checkpoints) do estado do grafo a cada passo.\n",
    "\n",
    "Checkpoints são salvos em uma thread, que pode ser acessada após a execução do grafo ser completada.\n",
    "\n",
    "![checkpointer](img/checkpoints.png)\n",
    "\n",
    "Compilamos o grafo com um [checkpointer](https://langchain-ai.github.io/langgraph/concepts/persistence/#checkpointer-libraries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a72377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[write_email],\n",
    "    prompt=\"Responda à solicitação do usuário usando as ferramentas fornecidas.\",\n",
    "    checkpointer=InMemorySaver()\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Quais são algumas boas práticas para escrever emails?\"}]}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10984007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter o snapshot de estado mais recente\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "state = agent.get_state(config)\n",
    "for message in state.values['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f23ac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue the conversation\n",
    "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Good, let's use lesson 3 to craft a response to my boss confirming that I want to attend Interrupt\"}]}, config)\n",
    "for m in result['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f09fe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue the conversation\n",
    "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"I like this, let's write the email to boss@company.ai\"}]}, config)\n",
    "for m in result['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae8a5d2-cf8a-4465-8d1b-96bb6c6276cf",
   "metadata": {},
   "source": [
    "#### Interrupts\n",
    "\n",
    "In LangGraph, we can also use [interrupts](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/wait-user-input/) to stop graph execution at specific points.\n",
    "\n",
    "Often this is used to collect input from a user and continue execution with collected input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c17b60-9474-49a5-b4a1-583b0dc8bba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "from langgraph.types import Command, interrupt\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "class State(TypedDict):\n",
    "    input: str\n",
    "    user_feedback: str\n",
    "\n",
    "def step_1(state):\n",
    "    print(\"---Step 1---\")\n",
    "    pass\n",
    "\n",
    "def human_feedback(state):\n",
    "    print(\"---human_feedback---\")\n",
    "    feedback = interrupt(\"Please provide feedback:\")\n",
    "    return {\"user_feedback\": feedback}\n",
    "\n",
    "def step_3(state):\n",
    "    print(\"---Step 3---\")\n",
    "    pass\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"step_1\", step_1)\n",
    "builder.add_node(\"human_feedback\", human_feedback)\n",
    "builder.add_node(\"step_3\", step_3)\n",
    "builder.add_edge(START, \"step_1\")\n",
    "builder.add_edge(\"step_1\", \"human_feedback\")\n",
    "builder.add_edge(\"human_feedback\", \"step_3\")\n",
    "builder.add_edge(\"step_3\", END)\n",
    "\n",
    "# Set up memory\n",
    "memory = InMemorySaver()\n",
    "\n",
    "# Add\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c94cb7-067c-4bc5-be33-b615d8e5775e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_graph(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028372b5-88ae-4f13-813d-22430a697f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "initial_input = {\"input\": \"hello world\"}\n",
    "\n",
    "# Thread\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Run the graph until the first interruption\n",
    "for event in graph.stream(initial_input, thread, stream_mode=\"updates\"):\n",
    "    print(event)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f142d467-7b4c-4a04-bad5-bf3fbe953b84",
   "metadata": {},
   "source": [
    "Para retomar de uma interrupção, podemos usar [o objeto `Command`](https://langchain-ai.github.io/langgraph/how-tos/command/). \n",
    "\n",
    "Vamos usá-lo para retomar o grafo do estado interrompido, passando o valor para retornar da chamada de interrupção para `resume`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90374d3-65a1-4658-82ee-a16cc2835cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue the graph execution\n",
    "for event in graph.stream(\n",
    "    Command(resume=\"go to step 3!\"),\n",
    "    thread,\n",
    "    stream_mode=\"updates\",\n",
    "):\n",
    "    print(event)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8639a518",
   "metadata": {},
   "source": [
    "### Tracing\n",
    "\n",
    "When we are using LangChain or LangGraph, LangSmith logging [will work out of the box](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph) with the following environment variables set:\n",
    "\n",
    "```\n",
    "export LANGSMITH_TRACING=true\n",
    "export LANGSMITH_API_KEY=\"<your-langsmith-api-key>\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb2a3e5",
   "metadata": {},
   "source": [
    "Here is the LangSmith trace from above agent execution:\n",
    "\n",
    "https://smith.langchain.com/public/6f77014f-d054-44ed-aa2c-8b06ceab689f/r\n",
    "\n",
    "We can see that the agent is able to continue the conversation from the previous state because we used a checkpointer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0269214",
   "metadata": {},
   "source": [
    "Também podemos implantar nosso grafo usando a [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/). \n",
    "\n",
    "Isso cria um servidor [com uma API](https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref.html) que podemos usar para interagir com nosso grafo e um IDE interativo, o LangGraph [Studio](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/).\n",
    "\n",
    "Precisamos apenas garantir que nosso projeto tenha [uma estrutura](https://langchain-ai.github.io/langgraph/concepts/application_structure/) como esta:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3644093",
   "metadata": {},
   "source": [
    "Here we can see a visualization of the graph as well as the graph state in Studio.\n",
    "\n",
    "![langgraph_studio](img/langgraph_studio.png)\n",
    "\n",
    "Also, you can see API docs for the local deployment here:\n",
    "\n",
    "http://127.0.0.1:2024/docs"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
